# Binance Trading Bot with Rust & ONNX

This project is a realâ€‘time cryptocurrency trading bot that streams data from Binance, performs feature engineering, runs an ONNX machine learning model, and executes trades based on the model's predictions. The core is written in Rust for maximum performance and low latency.

## âœ¨ Key Features

- **Historical data preload** â€“ On startup, the bot automatically fetches the latest 50,000 M15 candlesticks (or loads from a Parquet cache) to provide context for feature engineering.
- **Efficient memory management** â€“ The rolling window is implemented as a ring buffer (`VecDeque`), avoiding the O(n) cost of shifting elements when old candles are removed. A separate feature window is no longer kept; features are computed directly from a slice of the main window, eliminating duplication of 50,000 candles.
- **Rich feature engineering** â€“ Computes:
  - **EMA50 and EMA200** for three timeframes (M15, H1, H4) using M15 candle data. Higherâ€‘timeframe EMAs are resampled and forwardâ€‘filled so that every M15 row has the most recent H1 and H4 values. Nulls are left in the earliest rows where insufficient data exists.
  - **Pivot strength** â€“ For each candle, counts consecutive candles to the left and right that satisfy pivot conditions:
    - `pivot_high_left` / `pivot_high_right` â€“ number of previous/next candles with **high < current high** (lower highs).
    - `pivot_low_left` / `pivot_low_right` â€“ number of previous/next candles with **low > current low** (higher lows).
    - `pivot_high_strength` / `pivot_low_strength` â€“ the **minimum** of left and right counts, indicating the symmetric strength of the pivot.
- **Data persistence** â€“ Newly arriving closed candles are persisted in several ways:
  - **Raw data log** â€“ Appended to a CSV file (e.g., `m5_latest_50000_raw.csv`) for a permanent, everâ€‘growing log of all raw candles.
  - **Feature logs** â€“ Two appendâ€‘only CSV files store the computed feature row for every live candle:  
    - `m5_features.csv` â€“ a permanent feature history.  
    - `m5_streaming_features.csv` â€“ an additional copy (useful for separate pipelines).
  - **Rolling Parquet cache** â€“ The raw rolling window (latest 50,000 candles) is overwritten to a Parquet file (e.g., `m5_latest_50000.parquet`) after each new candle, ensuring fast restarts.
  - **Full feature Parquet** â€“ The enriched feature DataFrame (50,000 rows) is overwritten to a Parquet file (e.g., `m5_features.parquet`) after each new candle, providing a quickâ€‘restart snapshot for the ML model.
- **Optimised CSV writing** â€“ Feature rows are appended using a rowâ€‘byâ€‘row writer that converts timestamps to humanâ€‘readable strings on the fly, avoiding large memory allocations.
- **Asynchronous I/O** â€“ All disk writes are offloaded to background threads using `tokio::spawn_blocking` and awaited at the end of each message cycle. This prevents blocking the WebSocket event loop and keeps latency low.
- **Modular codebase** â€“ Separated into logical modules (`binance_client`, `data_storage`, `kline`, `live_stream`, `features/`) for maintainability and testability.
- **Developmentâ€‘friendly** â€“ Uses `cargo watch` inside Docker for live code reloading, with proper environment variable handling and fileâ€‘watching exclusions.

## ğŸ—ï¸ System Architecture

```mermaid
flowchart TD
    A[Binance WebSocket\nMarket Data] --> B[Data Ingest\n& Normalization]
    A2[Binance REST API\nHistorical Klines] --> B2[Historical Data Fetcher]
    B2 --> C2[Parquet Cache\n(Rolling Window)]
    C2 --> D[Feature Engineering\n& Windowing]
    B --> D
    D --> E[Feature DataFrame\n(50k window, EMAs + Pivots added)]
    E --> F[ONNX Runtime\nML Inference]
    E --> Parquet_Features[Parquet\nFeatures Snapshot]
    E --> CSV_Features_Log[CSV\nFeatures Log]
    E --> CSV_Streaming_Log[CSV\nStreaming Features Log]
    F --> G[Trading Decision\n& Risk Management]
    G --> H[Binance REST API\nOrder Execution]
    H --> I[Portfolio State\n& Order Management]
    I --> D
    B --> CSV_Raw_Log[CSV\nRaw Data Log]
```

## ğŸ“ Project Structure

```
binance_trading_bot/
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ Cargo.lock
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env.example
â”œâ”€â”€ Dockerfile              # Production image
â”œâ”€â”€ Dockerfile.dev          # Development image with cargo-watch
â”œâ”€â”€ docker-compose.yml      # Development compose with live reload
â”œâ”€â”€ docker-compose.prod.yml # Production compose
â”œâ”€â”€ README.md
â””â”€â”€ src/
    â”œâ”€â”€ main.rs             # CLI entry point
    â”œâ”€â”€ binance_client.rs   # REST API client for historical klines
    â”œâ”€â”€ data_storage.rs     # Parquet/CSV I/O and DataFrame conversions (sync + async wrappers)
    â”œâ”€â”€ kline.rs            # Kline struct and deserialization
    â”œâ”€â”€ live_stream.rs      # WebSocket streaming logic with async saves
    â””â”€â”€ features/           # Feature engineering module
        â”œâ”€â”€ mod.rs          # Main entry point, orchestrates feature computation
        â”œâ”€â”€ ema.rs          # EMA calculations (M15, H1, H4)
        â””â”€â”€ pivots.rs       # Pivot strength calculations
```

## ğŸš€ Getting Started

### Prerequisites

- Rust (edition 2021) â€“ for local builds.
- Docker & Docker Compose â€“ for containerized development/production.

### Installation (Local)

1. Clone the repository:

   ```bash
   git clone https://github.com/yourname/binance_trading_bot.git
   cd binance_trading_bot
   ```

2. Build the project:

   ```bash
   cargo build --release
   ```

### Usage

#### Live Streaming

```bash
# Live trade data (default)
cargo run

# 5â€‘minute candlesticks
cargo run m5

# 15â€‘minute candlesticks
cargo run m15
```

On startup, the bot loads (or fetches) the latest 50,000 candles from Binance and stores them in a rolling window (implemented as a ring buffer). For every new closed candle received via WebSocket:

- The candle is appended to the **raw CSV log** (e.g., `data/m5_latest_50000_raw.csv`).
- The inâ€‘memory rolling window is updated (oldest removed, newest added) and the updated window is saved to a **raw Parquet cache** (overwrite).
- Features are computed on the most recent 50,000 candles.
- The enriched feature DataFrame is saved to a **full feature Parquet** (overwrite) for fast restart.
- The latest feature row is appended to **two feature logs**:  
  - `data/m5_features.csv` â€“ permanent feature history.  
  - `data/m5_streaming_features.csv` â€“ a second copy (can be used for separate pipelines).

All disk writes are performed asynchronously using background threads, and the main loop waits for them to complete before processing the next message (ensuring no backlog and consistent state). With the heavy fullâ€‘CSV overwrites removed, perâ€‘message latency is typically under 400â€¯ms â€“ well within the 5â€‘ or 15â€‘minute candle interval.

#### Fetch Historical Data Manually

```bash
cargo run fetch-historical 15m 2025-01-01 2025-01-31 data/jan2025.parquet
```

This fetches the specified date range, saves it as Parquet, and also creates a CSV file in the same directory (without affecting the live rolling window files). The Parquet will contain a `row_number` column.

### Docker Development (with live reload)

We provide a `docker-compose.dev.yml` for development that uses `cargo watch` to automatically rebuild and restart on code changes.

```bash
# Start the dev container (default stream: m5)
docker-compose -f docker-compose.dev.yml up

# Override stream type (e.g., trade)
STREAM_TYPE=trade docker-compose -f docker-compose.dev.yml up
```

- The container mounts your local source code, so changes are reflected immediately.
- Generated CSV/Parquet files are written to the containerâ€™s filesystem; to access them on the host, mount a volume (e.g., `./data:/app/data`).
- `cargo watch` is configured to ignore `data/` and `*.csv` to avoid infinite restart loops.

### Docker Production

```bash
# Build the production image
docker build -t binance-streamer .

# Run with default trade stream
docker run --rm binance-streamer

# Run with 5m candles
docker run --rm binance-streamer m5
```

## ğŸ§© Modules Explained

### `kline.rs`

Defines the `Kline` struct and custom deserializer for Binanceâ€™s arrayâ€‘based kline format.

### `binance_client.rs`

Contains two public functions:

- `fetch_klines_range` â€“ fetches klines between two timestamps with automatic pagination.
- `fetch_latest_klines` â€“ fetches the most recent N candles for a given interval.

### `data_storage.rs`

Provides synchronous functions for DataFrame conversion, Parquet/CSV I/O, and appending rows. Also exports async wrappers that use `tokio::task::spawn_blocking` to offload blocking I/O to background threads.

### `features/` module

- **`mod.rs`** â€“ Orchestrates feature computation: converts klines to DataFrame, adds datetime, and calls EMA and pivot routines.
- **`ema.rs`** â€“ Implements EMA50/200 for M15, H1, H4 (resampling, joins, forwardâ€‘fill).
- **`pivots.rs`** â€“ Implements pivot strength calculations as described above.

### `live_stream.rs`

Manages the WebSocket connection, parses incoming messages, and maintains **a single rolling window** of up to 50,000 candles (the â€œraw windowâ€) as a `VecDeque`. On each new closed candle:

- The candle is added to the window (and the oldest is removed if capacity is exceeded).
- A **temporary slice** of the most recent 50,000 candles is collected and passed to the feature engine.
- Features are computed, producing a full feature DataFrame.
- The following save operations are spawned as background tasks (using `tokio::spawn` + async wrappers):
  - **Feature Parquet** (overwrite)
  - **Feature log CSV** (append)
  - **Streaming feature log CSV** (append)
  - **Raw kline CSV** (append)
  - **Raw Parquet cache** (overwrite) â€“ may be batched to reduce I/O.
- The bot then waits for all spawned tasks to complete (`join_all`) before proceeding to the next message. This ensures that all files are updated before the next candle is processed, while still allowing concurrency during I/O.

This design keeps the WebSocket loop responsive and guarantees data consistency.

### `main.rs`

Parses CLI arguments and orchestrates:

- `fetch-historical` subcommand.
- Default live mode: determines the interval and file paths based on the stream type, loads historical data (from cache or network), converts it to a `VecDeque`, and starts the live stream.

## ğŸ’¾ File Contents Summary

| File                                | Type     | Contains `row_number` | Update Pattern               | Purpose                               |
|-------------------------------------|----------|----------------------|------------------------------|---------------------------------------|
| `m5_latest_50000_raw.csv`           | CSV      | âŒ                   | Append (every message)       | Permanent raw data log                |
| `m5_latest_50000.parquet`           | Parquet  | âœ… (1..N)            | Overwrite (every message)    | Fast restart cache (raw window)       |
| `m5_features.parquet`               | Parquet  | âœ… (1..N)            | Overwrite (every message)    | Fast restart snapshot (feature window)|
| `m5_features.csv`                   | CSV      | âœ… (1..N)            | Append (every message)       | Permanent feature log                 |
| `m5_streaming_features.csv`         | CSV      | âœ… (1..N)            | Append (every message)       | Additional feature log (optional)     |

(Similarly for `m15`.)

- The `row_number` column in Parquet files is windowâ€‘relative (1..50,000 for raw, 1..50,000 for features) and resets on each overwrite.
- Appendâ€‘only CSV logs do **not** contain a `row_number` column (the row number can be added on load if needed).

## ğŸ§ª Development Roadmap

- [x] WebSocket streaming from Binance (trades, 5m, 15m) with readable timestamps
- [x] Docker containerization (production & development)
- [x] Historical data fetching and caching (Parquet)
- [x] Modular code structure
- [x] Live data persistence (CSV append + Parquet rolling window)
- [x] Feature engineering (EMA50/200 for M15, H1, H4)
- [x] Pivot strength features
- [x] Feature persistence (Parquet + appendâ€‘only CSV logs)
- [x] Memory optimisations (ring buffer, no duplicate feature window)
- [x] Asynchronous I/O with background threads
- [ ] Add more streams (order book, other intervals)
- [ ] Integrate ONNX Runtime for inference
- [ ] Build decision engine with risk management
- [ ] Add order execution (testnet first)
- [ ] Live paper trading mode
- [ ] Realâ€‘money trading with safeguards

## âš™ï¸ Environment Variables

- `STREAM_TYPE` â€“ Sets the default stream for development (`trade`, `m5`, `m15`). Used in `docker-compose.dev.yml`.
- `RUST_BACKTRACE=1` â€“ Enables full backtraces on panics.

## ğŸ“š Dependencies

- `tokio` â€“ async runtime
- `tokio-tungstenite` â€“ WebSocket client
- `serde_json` â€“ JSON parsing
- `chrono` â€“ humanâ€‘readable timestamps
- `reqwest` â€“ REST API client
- `polars` â€“ DataFrame operations, Parquet/CSV I/O, resampling, EWMA, joins, forwardâ€‘fill
- `anyhow` â€“ flexible error handling
- `futures-util` â€“ stream utilities and `join_all`
- `url` â€“ URL parsing
- (Future) `ort` â€“ ONNX Runtime bindings
- (Future) `ccxtâ€‘rust` â€“ exchange connectivity

## âš ï¸ Risk Disclaimer

Trading cryptocurrencies carries significant risk. This software is for educational purposes only. Use at your own risk. Always test thoroughly on testnet before using real funds.
