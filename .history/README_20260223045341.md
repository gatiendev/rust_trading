# Binance Trading Bot with Rust & ONNX

This project is a realâ€‘time cryptocurrency trading bot that streams data from Binance, performs feature engineering, runs an ONNX machine learning model, and executes trades based on the model's predictions. The core is written in Rust for maximum performance and low latency.

## âœ¨ New Features

- **Historical data preload** â€“ On startup, the bot automatically fetches the latest 50,000 M15 candlesticks (or loads from a Parquet cache) to provide context for feature engineering.
- **Live data persistence** â€“ Newly arriving closed candles are merged into an inâ€‘memory rolling window (up to 50,000 candles) and persisted in two ways:
  - **Appended to a CSV file** (e.g., `m5_latest_50000_raw.csv`) for a permanent, everâ€‘growing log of all candles.
  - **Overwritten in a Parquet file** (e.g., `m5_latest_50000.parquet`) after each new candle, ensuring the cache always contains the latest 50,000 candles for fast reloading.
- **Feature engineering** â€“ Computes **EMA50 and EMA200** for three timeframes (M15, H1, H4) using the M15 candle data. Higherâ€‘timeframe EMAs are resampled and forwardâ€‘filled so that every M15 row has the most recent H1 and H4 values. Nulls are left in the earliest rows where insufficient data exists.
- **Feature persistence** â€“ The enriched feature DataFrame (raw kline columns plus six EMAs) is saved as both Parquet and CSV (e.g., `m5_features.parquet` and `m5_features.csv`), updated after each new candle.
- **Modular codebase** â€“ Separated into logical modules (`binance_client`, `data_storage`, `kline`, `live_stream`, `features`) for maintainability and testability.
- **Developmentâ€‘friendly** â€“ Uses `cargo watch` inside Docker for live code reloading, with proper environment variable handling and fileâ€‘watching exclusions.

## ğŸ—ï¸ System Architecture

```mermaid
flowchart TD
    A[Binance WebSocket\nMarket Data] --> B[Data Ingest\n& Normalization]
    A2[Binance REST API\nHistorical Klines] --> B2[Historical Data Fetcher]
    B2 --> C2[Parquet Cache\n(Rolling Window)]
    C2 --> D[Feature Engineering\n& Windowing]
    B --> D
    D --> E[Feature DataFrame\n(7k window, EMAs added)]
    E --> F[ONNX Runtime\nML Inference]
    E --> CSV_Features[CSV/Parquet\nFeatures Persistence]
    F --> G[Trading Decision\n& Risk Management]
    G --> H[Binance REST API\nOrder Execution]
    H --> I[Portfolio State\n& Order Management]
    I --> D
    B --> CSV_Raw[CSV Append Log\n(Permanent History)]
```

## ğŸ“ Project Structure

```
binance_trading_bot/
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ Cargo.lock
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env.example
â”œâ”€â”€ Dockerfile              # Production image
â”œâ”€â”€ Dockerfile.dev          # Development image with cargo-watch
â”œâ”€â”€ docker-compose.yml      # Development compose with live reload
â”œâ”€â”€ docker-compose.prod.yml # Production compose
â”œâ”€â”€ README.md
â””â”€â”€ src/
    â”œâ”€â”€ main.rs             # CLI entry point
    â”œâ”€â”€ binance_client.rs   # REST API client for historical klines
    â”œâ”€â”€ data_storage.rs     # Parquet/CSV I/O and DataFrame conversions
    â”œâ”€â”€ kline.rs            # Kline struct and deserialization
    â”œâ”€â”€ live_stream.rs      # WebSocket streaming logic
    â””â”€â”€ features.rs         # Feature engineering (EMAs, resampling)
```

## ğŸš€ Getting Started

### Prerequisites

- Rust (edition 2021) â€“ for local builds.
- Docker & Docker Compose â€“ for containerized development/production.

### Installation (Local)

1. Clone the repository:

   ```bash
   git clone https://github.com/yourname/binance_trading_bot.git
   cd binance_trading_bot
   ```

2. Build the project:

   ```bash
   cargo build --release
   ```

### Usage

#### Live Streaming

```bash
# Live trade data (default)
cargo run

# 5â€‘minute candlesticks
cargo run m5

# 15â€‘minute candlesticks
cargo run m15
```

On startup, the bot loads (or fetches) the latest 50,000 candles from Binance and stores them in a rolling window. For every new closed candle received via WebSocket:

- The candle is appended to a raw CSV file (e.g., `data/m5_latest_50000_raw.csv`).
- The raw rolling window (50k candles) is updated and saved to a Parquet file (e.g., `data/m5_latest_50000.parquet`), overwriting the previous version.
- A **feature window** of the most recent 7,000 candles is maintained.
- Feature engineering computes EMA50 and EMA200 for M15, H1, and H4 (using resampling and forwardâ€‘fill) on the feature window.
- The enriched feature DataFrame is saved to both Parquet and CSV (e.g., `data/m5_features.parquet` and `data/m5_features.csv`), overwritten each time.

#### Fetch Historical Data Manually

```bash
cargo run fetch-historical 15m 2025-01-01 2025-01-31 data/jan2025.parquet
```

This fetches the specified date range, saves it as Parquet, and also creates a CSV file in the same directory (without affecting the live rolling window files).

### Docker Development (with live reload)

We provide a `docker-compose.dev.yml` for development that uses `cargo watch` to automatically rebuild and restart on code changes.

```bash
# Start the dev container (default stream: m5)
docker-compose -f docker-compose.dev.yml up

# Override stream type (e.g., trade)
STREAM_TYPE=trade docker-compose -f docker-compose.dev.yml up
```

- The container mounts your local source code, so changes are reflected immediately.
- Generated CSV/Parquet files are written to the containerâ€™s filesystem; to access them on the host, mount a volume (e.g., `./data:/app/data`).
- `cargo watch` is configured to ignore `data/` and `*.csv` to avoid infinite restart loops.

### Docker Production

```bash
# Build the production image
docker build -t binance-streamer .

# Run with default trade stream
docker run --rm binance-streamer

# Run with 5m candles
docker run --rm binance-streamer m5
```

## ğŸ§© Modules Explained

### `kline.rs`

Defines the `Kline` struct and custom deserializer for Binanceâ€™s arrayâ€‘based kline format.

### `binance_client.rs`

Contains two public functions:

- `fetch_klines_range` â€“ fetches klines between two timestamps with automatic pagination.
- `fetch_latest_klines` â€“ fetches the most recent N candles for a given interval.

### `data_storage.rs`

Handles conversion between `Vec<Kline>` and Polars `DataFrame`, and provides functions to:

- Save/load a full slice of klines as Parquet (overwrite).
- Append a single kline to a CSV file (creates headers if file is new).
- Write a full slice of klines to a CSV (used for initial baseline).
- Save a DataFrame to Parquet or CSV (overwrite) â€“ used for feature persistence.

### `features.rs`

Implements feature engineering:

- Converts a slice of klines to a DataFrame and adds a datetime column.
- Computes EMA50 and EMA200 directly on the M15 close prices.
- Resamples the M15 data to 1â€‘hour and 4â€‘hour intervals, computes EMAs on the resampled closes.
- Leftâ€‘joins the resampled EMAs back to the original M15 timestamps.
- Forwardâ€‘fills the EMA columns so every M15 row has the most recent H1 and H4 values.
- Returns a DataFrame with original kline columns plus six EMA columns (`ema50_m15`, `ema200_m15`, `ema50_h1`, `ema200_h1`, `ema50_h4`, `ema200_h4`).

### `live_stream.rs`

Manages the WebSocket connection, parses incoming messages, and maintains two rolling windows:

- **Raw window** â€“ last 50,000 candles (used for Parquet cache and longâ€‘term context).
- **Feature window** â€“ last 7,000 candles (used for feature computation).

On each new closed candle, it updates both windows, recomputes features, saves the feature DataFrame to Parquet/CSV, appends the raw candle to the CSV log, and overwrites the raw Parquet cache.

### `main.rs`

Parses CLI arguments and orchestrates:

- `fetch-historical` subcommand.
- Default live mode: determines the interval and file paths based on the stream type, loads historical data (from cache or network), and starts the live stream.

## ğŸ§ª Development Roadmap

- [x] WebSocket streaming from Binance (trades, 5m, 15m) with readable timestamps
- [x] Docker containerization (production & development)
- [x] Historical data fetching and caching (Parquet)
- [x] Modular code structure
- [x] Live data persistence (CSV append + Parquet rolling window)
- [x] Feature engineering (EMA50/200 for M15, H1, H4)
- [x] Feature persistence (Parquet/CSV)
- [ ] Add more streams (order book, other intervals)
- [ ] Integrate ONNX Runtime for inference
- [ ] Build decision engine with risk management
- [ ] Add order execution (testnet first)
- [ ] Live paper trading mode
- [ ] Realâ€‘money trading with safeguards

## âš™ï¸ Environment Variables

- `STREAM_TYPE` â€“ Sets the default stream for development (`trade`, `m5`, `m15`). Used in `docker-compose.dev.yml`.
- `RUST_BACKTRACE=1` â€“ Enables full backtraces on panics.

## ğŸ“š Dependencies

- `tokio` â€“ async runtime
- `tokio-tungstenite` â€“ WebSocket client
- `serde_json` â€“ JSON parsing
- `chrono` â€“ humanâ€‘readable timestamps
- `reqwest` â€“ REST API client
- `polars` â€“ DataFrame operations, Parquet/CSV I/O, resampling, EWMA, joins, forwardâ€‘fill
- `anyhow` â€“ flexible error handling
- `futures-util` â€“ stream utilities
- `url` â€“ URL parsing
- (Future) `ort` â€“ ONNX Runtime bindings
- (Future) `ccxtâ€‘rust` â€“ exchange connectivity

## âš ï¸ Risk Disclaimer

Trading cryptocurrencies carries significant risk. This software is for educational purposes only. Use at your own risk. Always test thoroughly on testnet before using real funds.
