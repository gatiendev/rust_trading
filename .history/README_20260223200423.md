# Binance Trading Bot with Rust & ONNX

This project is a realâ€‘time cryptocurrency trading bot that streams data from Binance, performs feature engineering, runs an ONNX machine learning model, and executes trades based on the model's predictions. The core is written in Rust for maximum performance and low latency.

## âœ¨ Key Features

- **Historical data preload** â€“ On startup, the bot automatically fetches the latest 50,000 M15 candlesticks (or loads from a Parquet cache) to provide context for feature engineering.
- **Live data persistence** â€“ Newly arriving closed candles are merged into an inâ€‘memory rolling window (up to 50,000 candles) and persisted in two ways:
  - **Appended to a CSV file** (e.g., `m5_latest_50000_raw.csv`) for a permanent, everâ€‘growing log of all candles.
  - **Overwritten in a Parquet file** (e.g., `m5_latest_50000.parquet`) after each new candle, ensuring the cache always contains the latest 50,000 candles for fast reloading.
- **Efficient memory management** â€“ The rolling window is implemented as a ring buffer (`VecDeque`), avoiding the O(n) cost of shifting elements when old candles are removed. A separate feature window is no longer kept; features are computed directly from a slice of the main window, eliminating duplication of 7,000 candles.
- **Rich feature engineering** â€“ Computes:
  - **EMA50 and EMA200** for three timeframes (M15, H1, H4) using M15 candle data. Higherâ€‘timeframe EMAs are resampled and forwardâ€‘filled so that every M15 row has the most recent H1 and H4 values. Nulls are left in the earliest rows where insufficient data exists.
  - **Pivot strength** â€“ For each candle, counts consecutive candles to the left and right that satisfy pivot conditions:
    - `pivot_high_left` / `pivot_high_right` â€“ number of previous/next candles with **high < current high** (lower highs).
    - `pivot_low_left` / `pivot_low_right` â€“ number of previous/next candles with **low > current low** (higher lows).
    - `pivot_high_strength` / `pivot_low_strength` â€“ the **minimum** of left and right counts, indicating the symmetric strength of the pivot.
- **Feature persistence** â€“ The enriched feature DataFrame (raw kline columns plus six EMAs and six pivot columns) is saved as both Parquet and CSV (e.g., `m5_features.parquet` and `m5_features.csv`), updated after each new candle. All persisted DataFrames (raw Parquet, feature Parquet/CSV) include a **windowâ€‘relative `row_number` column** starting at 1, providing a convenient index for machine learning pipelines.
- **Optimised CSV writing** â€“ The feature CSV is written row by row without cloning the entire DataFrame, reducing peak memory usage.
- **Modular codebase** â€“ Separated into logical modules (`binance_client`, `data_storage`, `kline`, `live_stream`, `features/`) for maintainability and testability.
- **Developmentâ€‘friendly** â€“ Uses `cargo watch` inside Docker for live code reloading, with proper environment variable handling and fileâ€‘watching exclusions.

## ğŸ—ï¸ System Architecture

```mermaid
flowchart TD
    A[Binance WebSocket\nMarket Data] --> B[Data Ingest\n& Normalization]
    A2[Binance REST API\nHistorical Klines] --> B2[Historical Data Fetcher]
    B2 --> C2[Parquet Cache\n(Rolling Window)]
    C2 --> D[Feature Engineering\n& Windowing]
    B --> D
    D --> E[Feature DataFrame\n(7k window, EMAs + Pivots added)]
    E --> F[ONNX Runtime\nML Inference]
    E --> CSV_Features[CSV/Parquet\nFeatures Persistence]
    F --> G[Trading Decision\n& Risk Management]
    G --> H[Binance REST API\nOrder Execution]
    H --> I[Portfolio State\n& Order Management]
    I --> D
    B --> CSV_Raw[CSV Append Log\n(Permanent History)]
```

## ğŸ“ Project Structure

```
binance_trading_bot/
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ Cargo.lock
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env.example
â”œâ”€â”€ Dockerfile              # Production image
â”œâ”€â”€ Dockerfile.dev          # Development image with cargo-watch
â”œâ”€â”€ docker-compose.yml      # Development compose with live reload
â”œâ”€â”€ docker-compose.prod.yml # Production compose
â”œâ”€â”€ README.md
â””â”€â”€ src/
    â”œâ”€â”€ main.rs             # CLI entry point
    â”œâ”€â”€ binance_client.rs   # REST API client for historical klines
    â”œâ”€â”€ data_storage.rs     # Parquet/CSV I/O and DataFrame conversions
    â”œâ”€â”€ kline.rs            # Kline struct and deserialization
    â”œâ”€â”€ live_stream.rs      # WebSocket streaming logic
    â””â”€â”€ features/           # Feature engineering module
        â”œâ”€â”€ mod.rs          # Main entry point, orchestrates feature computation
        â”œâ”€â”€ ema.rs          # EMA calculations (M15, H1, H4)
        â””â”€â”€ pivots.rs       # Pivot strength calculations
```

## ğŸš€ Getting Started

### Prerequisites

- Rust (edition 2021) â€“ for local builds.
- Docker & Docker Compose â€“ for containerized development/production.

### Installation (Local)

1. Clone the repository:

   ```bash
   git clone https://github.com/yourname/binance_trading_bot.git
   cd binance_trading_bot
   ```

2. Build the project:

   ```bash
   cargo build --release
   ```

### Usage

#### Live Streaming

```bash
# Live trade data (default)
cargo run

# 5â€‘minute candlesticks
cargo run m5

# 15â€‘minute candlesticks
cargo run m15
```

On startup, the bot loads (or fetches) the latest 50,000 candles from Binance and stores them in a rolling window (implemented as a ring buffer). For every new closed candle received via WebSocket:

- The candle is appended to a raw CSV file (e.g., `data/m5_latest_50000_raw.csv`). **This file does not contain a row number** to keep appends simple and efficient.
- The raw rolling window (50k candles) is updated (oldest removed, newest added) and saved to a Parquet file (e.g., `data/m5_latest_50000.parquet`), overwriting the previous version. This Parquet includes a `row_number` column (1..50,000) relative to the current window.
- **Feature engineering** is performed on the most recent 7,000 candles taken directly from the raw window (no separate copy is kept).
- The enriched feature DataFrame is saved to both Parquet and CSV (e.g., `data/m5_features.parquet` and `data/m5_features.csv`), overwritten each time. Both files contain a `row_number` column (1..7,000) relative to the feature window.

#### Fetch Historical Data Manually

```bash
cargo run fetch-historical 15m 2025-01-01 2025-01-31 data/jan2025.parquet
```

This fetches the specified date range, saves it as Parquet, and also creates a CSV file in the same directory (without affecting the live rolling window files). The Parquet will contain a `row_number` column.

### Docker Development (with live reload)

We provide a `docker-compose.dev.yml` for development that uses `cargo watch` to automatically rebuild and restart on code changes.

```bash
# Start the dev container (default stream: m5)
docker-compose -f docker-compose.dev.yml up

# Override stream type (e.g., trade)
STREAM_TYPE=trade docker-compose -f docker-compose.dev.yml up
```

- The container mounts your local source code, so changes are reflected immediately.
- Generated CSV/Parquet files are written to the containerâ€™s filesystem; to access them on the host, mount a volume (e.g., `./data:/app/data`).
- `cargo watch` is configured to ignore `data/` and `*.csv` to avoid infinite restart loops.

### Docker Production

```bash
# Build the production image
docker build -t binance-streamer .

# Run with default trade stream
docker run --rm binance-streamer

# Run with 5m candles
docker run --rm binance-streamer m5
```

## ğŸ§© Modules Explained

### `kline.rs`

Defines the `Kline` struct and custom deserializer for Binanceâ€™s arrayâ€‘based kline format.

### `binance_client.rs`

Contains two public functions:

- `fetch_klines_range` â€“ fetches klines between two timestamps with automatic pagination.
- `fetch_latest_klines` â€“ fetches the most recent N candles for a given interval.

### `data_storage.rs`

Handles conversion between `Vec<Kline>` and Polars `DataFrame`, and provides functions to:

- Save/load a full slice of klines as Parquet (overwrite) â€“ includes a `row_number` column.
- Append a single kline to a CSV file (creates headers if file is new) â€“ the raw CSV does **not** include a row number.
- Write a full slice of klines to a CSV (used for initial baseline).
- Save a DataFrame to Parquet (overwrite).
- **Save a DataFrame to CSV without cloning** â€“ writes row by row, converting timestamp columns to humanâ€‘readable strings on the fly, avoiding a full copy of the feature DataFrame.

### `features/` module

- **`mod.rs`** â€“ Orchestrates feature computation: converts klines to DataFrame, adds datetime, and calls EMA and pivot routines.
- **`ema.rs`** â€“ Implements EMA50/200 for M15, H1, H4 (resampling, joins, forwardâ€‘fill).
- **`pivots.rs`** â€“ Implements pivot strength calculations as described above.

### `live_stream.rs`

Manages the WebSocket connection, parses incoming messages, and maintains **a single rolling window** of up to 50,000 candles (the â€œraw windowâ€). This window is implemented as a `VecDeque` (ring buffer) for efficient addition and removal. On each new closed candle:

- The new candle is appended to the back of the raw window; if the window exceeds 50,000 candles, the oldest is popped from the front â€“ both operations O(1).
- A **temporary slice** of the most recent 7,000 candles is created (via `iter().skip(...).collect()`) and passed to the feature engine.
- Features are computed, and the enriched DataFrame is saved to Parquet/CSV using the optimised writer (no cloning).
- The raw window is also written to a Parquet cache (overwrite) after being collected into a temporary `Vec` â€“ this copies 50,000 candles per iteration, which is acceptable for now and could be further optimised if needed.

#### Why `VecDeque`?

Originally, the raw window was stored in a `Vec` and old candles were removed with `remove(0)`, which shifts all remaining elements left â€“ an O(n) operation. With 50,000 candles, this caused significant CPU overhead and memory copying. Switching to `VecDeque` (a ring buffer) makes both adding to the back and removing from the front O(1) â€“ only a few pointers are updated, and no elements are moved. This drastically reduces the perâ€‘message processing time and eliminates unnecessary memory traffic.

Additionally, we removed the separate feature window (which held another 7,000 candles), cutting persistent memory usage by about 0.5â€¯MB and simplifying the code. Features are now derived from the main window on demand, using a temporary `Vec` that is freed after each iteration.

### `main.rs`

Parses CLI arguments and orchestrates:

- `fetch-historical` subcommand.
- Default live mode: determines the interval and file paths based on the stream type, loads historical data (from cache or network), converts it to a `VecDeque`, and starts the live stream.

## ğŸ’¾ File Contents Summary

| File                         | Type     | Contains `row_number` | Notes                          |
|------------------------------|----------|----------------------|--------------------------------|
| `m5_latest_50000_raw.csv`    | CSV      | âŒ                   | Appendâ€‘only, no index          |
| `m5_latest_50000.parquet`    | Parquet  | âœ… (1..N)            | Overwritten rolling window     |
| `m5_features.csv`            | CSV      | âœ… (1..N)            | Overwritten feature window     |
| `m5_features.parquet`        | Parquet  | âœ… (1..N)            | Overwritten feature window     |

(Similarly for `m15`.)

- The `row_number` column is windowâ€‘relative: for the raw Parquet it runs from 1 to 50,000 (most recent candle at the end); for feature files it runs from 1 to 7,000. This index resets each time the file is overwritten, which is ideal for ML training where each snapshot is a selfâ€‘contained dataset.
- If you ever need a permanent global index across all time (e.g., for the raw CSV), you can generate it onâ€‘theâ€‘fly when loading the data (e.g., with Polars' `with_row_index`).

## ğŸ§ª Development Roadmap

- [x] WebSocket streaming from Binance (trades, 5m, 15m) with readable timestamps
- [x] Docker containerization (production & development)
- [x] Historical data fetching and caching (Parquet)
- [x] Modular code structure
- [x] Live data persistence (CSV append + Parquet rolling window)
- [x] Feature engineering (EMA50/200 for M15, H1, H4)
- [x] Pivot strength features
- [x] Feature persistence (Parquet/CSV) with row number
- [x] Memory optimisations (ring buffer, no feature window, cloneâ€‘free CSV)
- [ ] Add more streams (order book, other intervals)
- [ ] Integrate ONNX Runtime for inference
- [ ] Build decision engine with risk management
- [ ] Add order execution (testnet first)
- [ ] Live paper trading mode
- [ ] Realâ€‘money trading with safeguards

## âš™ï¸ Environment Variables

- `STREAM_TYPE` â€“ Sets the default stream for development (`trade`, `m5`, `m15`). Used in `docker-compose.dev.yml`.
- `RUST_BACKTRACE=1` â€“ Enables full backtraces on panics.

## ğŸ“š Dependencies

- `tokio` â€“ async runtime
- `tokio-tungstenite` â€“ WebSocket client
- `serde_json` â€“ JSON parsing
- `chrono` â€“ humanâ€‘readable timestamps
- `reqwest` â€“ REST API client
- `polars` â€“ DataFrame operations, Parquet/CSV I/O, resampling, EWMA, joins, forwardâ€‘fill
- `anyhow` â€“ flexible error handling
- `futures-util` â€“ stream utilities
- `url` â€“ URL parsing
- (Future) `ort` â€“ ONNX Runtime bindings
- (Future) `ccxtâ€‘rust` â€“ exchange connectivity

## âš ï¸ Risk Disclaimer

Trading cryptocurrencies carries significant risk. This software is for educational purposes only. Use at your own risk. Always test thoroughly on testnet before using real funds.
