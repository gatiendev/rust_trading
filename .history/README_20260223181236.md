# Binance Trading Bot with Rust & ONNX

This project is a realâ€‘time cryptocurrency trading bot that streams data from Binance, performs feature engineering, runs an ONNX machine learning model, and executes trades based on the model's predictions. The core is written in Rust for maximum performance and low latency.

## âœ¨ Key Features

- **Historical data preload** â€“ On startup, the bot automatically fetches the latest 50,000 M15 candlesticks (or loads from a Parquet cache) to provide context for feature engineering.
- **Live data persistence** â€“ Newly arriving closed candles are merged into an inâ€‘memory rolling window (up to 50,000 candles) and persisted in two ways:
  - **Appended to a CSV file** (e.g., `m5_latest_50000_raw.csv`) for a permanent, everâ€‘growing log of all candles.
  - **Overwritten in a Parquet file** (e.g., `m5_latest_50000.parquet`) after each new candle, ensuring the cache always contains the latest 50,000 candles for fast reloading.
- **Rich feature engineering** â€“ Computes:
  - **EMA50 and EMA200** for three timeframes (M15, H1, H4) using M15 candle data. Higherâ€‘timeframe EMAs are resampled and forwardâ€‘filled so that every M15 row has the most recent H1 and H4 values. Nulls are left in the earliest rows where insufficient data exists.
  - **Pivot strength** â€“ For each candle, counts consecutive candles to the left and right that satisfy pivot conditions:
    - `pivot_high_left` / `pivot_high_right` â€“ number of previous/next candles with **high < current high** (lower highs).
    - `pivot_low_left` / `pivot_low_right` â€“ number of previous/next candles with **low > current low** (higher lows).
    - `pivot_high_strength` / `pivot_low_strength` â€“ the **minimum** of left and right counts, indicating the symmetric strength of the pivot.
- **Feature persistence** â€“ The enriched feature DataFrame (raw kline columns plus six EMAs and six pivot columns) is saved as both Parquet and CSV (e.g., `m5_features.parquet` and `m5_features.csv`), updated after each new candle. All persisted DataFrames (raw Parquet, feature Parquet/CSV) include a **windowâ€‘relative `row_number` column** starting at 1, providing a convenient index for machine learning pipelines.
- **Modular codebase** â€“ Separated into logical modules (`binance_client`, `data_storage`, `kline`, `live_stream`, `features/`) for maintainability and testability.
- **Developmentâ€‘friendly** â€“ Uses `cargo watch` inside Docker for live code reloading, with proper environment variable handling and fileâ€‘watching exclusions.

## ğŸ—ï¸ System Architecture

```mermaid
flowchart TD
    A[Binance WebSocket\nMarket Data] --> B[Data Ingest\n& Normalization]
    A2[Binance REST API\nHistorical Klines] --> B2[Historical Data Fetcher]
    B2 --> C2[Parquet Cache\n(Rolling Window)]
    C2 --> D[Feature Engineering\n& Windowing]
    B --> D
    D --> E[Feature DataFrame\n(7k window, EMAs + Pivots added)]
    E --> F[ONNX Runtime\nML Inference]
    E --> CSV_Features[CSV/Parquet\nFeatures Persistence]
    F --> G[Trading Decision\n& Risk Management]
    G --> H[Binance REST API\nOrder Execution]
    H --> I[Portfolio State\n& Order Management]
    I --> D
    B --> CSV_Raw[CSV Append Log\n(Permanent History)]
```

## ğŸ“ Project Structure

```
binance_trading_bot/
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ Cargo.lock
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env.example
â”œâ”€â”€ Dockerfile              # Production image
â”œâ”€â”€ Dockerfile.dev          # Development image with cargo-watch
â”œâ”€â”€ docker-compose.yml      # Development compose with live reload
â”œâ”€â”€ docker-compose.prod.yml # Production compose
â”œâ”€â”€ README.md
â””â”€â”€ src/
    â”œâ”€â”€ main.rs             # CLI entry point
    â”œâ”€â”€ binance_client.rs   # REST API client for historical klines
    â”œâ”€â”€ data_storage.rs     # Parquet/CSV I/O and DataFrame conversions
    â”œâ”€â”€ kline.rs            # Kline struct and deserialization
    â”œâ”€â”€ live_stream.rs      # WebSocket streaming logic
    â””â”€â”€ features/           # Feature engineering module
        â”œâ”€â”€ mod.rs          # Main entry point, orchestrates feature computation
        â”œâ”€â”€ ema.rs          # EMA calculations (M15, H1, H4)
        â””â”€â”€ pivots.rs       # Pivot strength calculations
```

## ğŸš€ Getting Started

### Prerequisites

- Rust (edition 2021) â€“ for local builds.
- Docker & Docker Compose â€“ for containerized development/production.

### Installation (Local)

1. Clone the repository:

   ```bash
   git clone https://github.com/yourname/binance_trading_bot.git
   cd binance_trading_bot
   ```

2. Build the project:

   ```bash
   cargo build --release
   ```

### Usage

#### Live Streaming

```bash
# Live trade data (default)
cargo run

# 5â€‘minute candlesticks
cargo run m5

# 15â€‘minute candlesticks
cargo run m15
```

On startup, the bot loads (or fetches) the latest 50,000 candles from Binance and stores them in a rolling window. For every new closed candle received via WebSocket:

- The candle is appended to a raw CSV file (e.g., `data/m5_latest_50000_raw.csv`). **This file does not contain a row number** to keep appends simple and efficient.
- The raw rolling window (50k candles) is updated and saved to a Parquet file (e.g., `data/m5_latest_50000.parquet`), overwriting the previous version. This Parquet includes a `row_number` column (1..50,000) relative to the current window.
- A **feature window** of the most recent 7,000 candles is maintained.
- Feature engineering computes EMAs and pivot strengths on the feature window.
- The enriched feature DataFrame is saved to both Parquet and CSV (e.g., `data/m5_features.parquet` and `data/m5_features.csv`), overwritten each time. Both files contain a `row_number` column (1..7,000) relative to the feature window.

#### Fetch Historical Data Manually

```bash
cargo run fetch-historical 15m 2025-01-01 2025-01-31 data/jan2025.parquet
```

This fetches the specified date range, saves it as Parquet, and also creates a CSV file in the same directory (without affecting the live rolling window files). The Parquet will contain a `row_number` column.

### Docker Development (with live reload)

We provide a `docker-compose.dev.yml` for development that uses `cargo watch` to automatically rebuild and restart on code changes.

```bash
# Start the dev container (default stream: m5)
docker-compose -f docker-compose.dev.yml up

# Override stream type (e.g., trade)
STREAM_TYPE=trade docker-compose -f docker-compose.dev.yml up
```

- The container mounts your local source code, so changes are reflected immediately.
- Generated CSV/Parquet files are written to the containerâ€™s filesystem; to access them on the host, mount a volume (e.g., `./data:/app/data`).
- `cargo watch` is configured to ignore `data/` and `*.csv` to avoid infinite restart loops.

### Docker Production

```bash
# Build the production image
docker build -t binance-streamer .

# Run with default trade stream
docker run --rm binance-streamer

# Run with 5m candles
docker run --rm binance-streamer m5
```

## ğŸ§© Modules Explained

### `kline.rs`

Defines the `Kline` struct and custom deserializer for Binanceâ€™s arrayâ€‘based kline format.

### `binance_client.rs`

Contains two public functions:

- `fetch_klines_range` â€“ fetches klines between two timestamps with automatic pagination.
- `fetch_latest_klines` â€“ fetches the most recent N candles for a given interval.

### `data_storage.rs`

Handles conversion between `Vec<Kline>` and Polars `DataFrame`, and provides functions to:

- Save/load a full slice of klines as Parquet (overwrite) â€“ **now includes a `row_number` column**.
- Append a single kline to a CSV file (creates headers if file is new) â€“ the raw CSV does **not** include a row number to keep appends simple.
- Write a full slice of klines to a CSV (used for initial baseline) â€“ also includes `row_number` when generated from a DataFrame.
- Save a DataFrame to Parquet or CSV (overwrite) â€“ used for feature persistence.

### `features/` module

- **`mod.rs`** â€“ Orchestrates feature computation: converts klines to DataFrame, adds datetime, and calls EMA and pivot routines.
- **`ema.rs`** â€“ Implements EMA50/200 for M15, H1, H4 (resampling, joins, forwardâ€‘fill).
- **`pivots.rs`** â€“ Implements pivot strength calculations as described above.

### `live_stream.rs`

Manages the WebSocket connection, parses incoming messages, and maintains two rolling windows:

- **Raw window** â€“ last 50,000 candles (used for Parquet cache and longâ€‘term context).
- **Feature window** â€“ last 7,000 candles (used for feature computation).

On each new closed candle, it updates both windows, recomputes features, saves the feature DataFrame (with `row_number`) to Parquet/CSV, appends the raw candle to the CSV log, and overwrites the raw Parquet cache (with `row_number`).

### `main.rs`

Parses CLI arguments and orchestrates:

- `fetch-historical` subcommand.
- Default live mode: determines the interval and file paths based on the stream type, loads historical data (from cache or network), and starts the live stream.

## ğŸ’¾ File Contents Summary

| File                         | Type     | Contains `row_number` | Notes                          |
|------------------------------|----------|----------------------|--------------------------------|
| `m5_latest_50000_raw.csv`    | CSV      | âŒ                   | Appendâ€‘only, no index          |
| `m5_latest_50000.parquet`    | Parquet  | âœ… (1..N)            | Overwritten rolling window     |
| `m5_features.csv`            | CSV      | âœ… (1..N)            | Overwritten feature window     |
| `m5_features.parquet`        | Parquet  | âœ… (1..N)            | Overwritten feature window     |

(Similarly for `m15`.)

- The `row_number` column is windowâ€‘relative: for the raw Parquet it runs from 1 to 50,000 (most recent candle at the end); for feature files it runs from 1 to 7,000. This index resets each time the file is overwritten, which is ideal for ML training where each snapshot is a selfâ€‘contained dataset.
- If you ever need a permanent global index across all time (e.g., for the raw CSV), you can generate it onâ€‘theâ€‘fly when loading the data (e.g., with Polars' `with_row_index`).

## ğŸ§ª Development Roadmap

- [x] WebSocket streaming from Binance (trades, 5m, 15m) with readable timestamps
- [x] Docker containerization (production & development)
- [x] Historical data fetching and caching (Parquet)
- [x] Modular code structure
- [x] Live data persistence (CSV append + Parquet rolling window)
- [x] Feature engineering (EMA50/200 for M15, H1, H4)
- [x] Pivot strength features
- [x] Feature persistence (Parquet/CSV) with row number
- [ ] Add more streams (order book, other intervals)
- [ ] Integrate ONNX Runtime for inference
- [ ] Build decision engine with risk management
- [ ] Add order execution (testnet first)
- [ ] Live paper trading mode
- [ ] Realâ€‘money trading with safeguards

## âš™ï¸ Environment Variables

- `STREAM_TYPE` â€“ Sets the default stream for development (`trade`, `m5`, `m15`). Used in `docker-compose.dev.yml`.
- `RUST_BACKTRACE=1` â€“ Enables full backtraces on panics.

## ğŸ“š Dependencies

- `tokio` â€“ async runtime
- `tokio-tungstenite` â€“ WebSocket client
- `serde_json` â€“ JSON parsing
- `chrono` â€“ humanâ€‘readable timestamps
- `reqwest` â€“ REST API client
- `polars` â€“ DataFrame operations, Parquet/CSV I/O, resampling, EWMA, joins, forwardâ€‘fill
- `anyhow` â€“ flexible error handling
- `futures-util` â€“ stream utilities
- `url` â€“ URL parsing
- (Future) `ort` â€“ ONNX Runtime bindings
- (Future) `ccxtâ€‘rust` â€“ exchange connectivity

## âš ï¸ Risk Disclaimer

Trading cryptocurrencies carries significant risk. This software is for educational purposes only. Use at your own risk. Always test thoroughly on testnet before using real funds.
